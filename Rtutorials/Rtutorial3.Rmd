---
title: "R Tutorial #3 -- Econ 103"
output:
  html_document:
    toc: true
    theme: readable
    css: custom.css
  pdf_document:
    toc: true
    highlight: zenburn
---

This tutorial is a bit of a grab-bag. The [first part](#part_one) shows you how to calculate the remaining summary statistics from the lectures and introduces a few new plotting commands. The [second part](#part_two) describes one of the most fundamental parts of R: creating your own functions. Finally, the [third part](#part_three) shows you how to import stock return data in R and introduces some related concepts and tools. Rather than appearing at the end, six exercises are scattered throughout the parts of this tutorial.

More Summary Statistics {#part_one}
--------------------------------------------------------
First we'll load the survey data from R Tutorial #2.

Like before we'll only need a subset of the columns available; rather than go to the hassle of deleting these columns afterwards, we can specify which columns to keep simultaneously with reading in the data using the `select` argument to `fread`. (There is also a `drop` argument for the reverse -- specifying columns to exclude)

```{r fread_select}
library(data.table)
survey = fread("http://www.ditraglia.com/econ103/old_survey.csv",
               select = c("handedness", "height", "handspan"))
```

### Correlation: `cor`

This command calculates sample correlation coefficients. If you pass it two vectors of the same length it returns the correlation between them. If you feed it a `data.table` or `matrix` it returns a matrix containing *all pairwise correlations* between the _columns_ (we haven't covered matrices in R since you'll only need to interpret them as output for this class -- for a primer on handling `matrix` objects, see [here](http://www.r-tutor.com/r-introduction/matrix)). Unlike the univariate summary statistics, in which we used `na.rm = TRUE` to ignore missing observations, for `cor` we use the argument `use = "complete.obs"`. The reason for this difference is that `cor` can handle a `matrix` or `data.table` as its input. In this case there are actually many different ways of handling missing observations. For more details, see `?cor`. Setting  `use = "complete.obs"` removes any rows with missing observations before proceeding.

```{r cor_use}
survey[ , cor(handspan, height, use = "complete.obs")]
```

Of course, we could use the approach we saw in R Tutorial #2 as well; whichever feels more natural:

```{r cor_isna}
survey[!is.na(handspan) & !is.na(height), cor(handspan, height)]
```

We can also feed the full `data.table` to `cor` like so:

```{r cor_dt}
cor(survey, use = "complete.obs")
#alternatively, there's the na.omit
#  function, which does just what it says:
cor(na.omit(survey))
```

We see that there is a large positive correlation between `height` and `handspan` and a slight positive correlation between `height` and `handedness.` There's basically no correlation between `handedness` and `handspan`. The preceding matrix has ones on its main diagonal since the correlation between any variable and itself is identically one. (A good exercise for extra practice would be to prove this assertion using the formula for correlation from class. It's not very difficult.)

If you look carefully, you'll notice that the correlation between `height` and `handspan` is slightly different when calculated at the same time as all the other correlations. This is because of the way that `use = "complete.obs"` drops rows. In this case, it indicates that there is at least one observation in our dataset for which we have `height` and `handspan` but not `handedness`.

For practice, we can find this with judicious use of `is.na` and logical operators:

```{r find_missing}
survey[is.na(handedness) & !is.na(height) & !is.na(handspan)]
```

### Covariance: `cov`

This command works just like `cor` but returns covariances rather than correlations:

```{r cov}
survey[ , cov(handspan, height, use = "complete.obs")]
cov(survey, use = "complete.obs")
cov(na.omit(survey))
```

### Regression: `lm` and `abline`

This command stands for "_l_inear _m_odel" and is R's general-purpose regression command. Its syntax is similar to that of `boxplot` from R Tutorial #2 in that we use a tilde (`~`) to indicate a functional relationship. Here the variable to the left of the tilde is the "y" variable, while the variable to the right is the "x" variable.

```{r lm}
survey[ , lm(height ~ handspan)]
```

Remember: unlike correlation and covariance, regression is *not symmetric*:

```{r lm_rev}
survey[ , lm(handspan ~ height)]
```

It turns out that we can use the *same syntax* with the command `plot`:

```{r plot_formula, results = 'hide'}
survey[ , plot(handspan ~ height)]
```

To add the regression line, we *follow* the `plot` command with the function `abline` like so:

```{r abline_lm, results = 'hide'}
survey[ , plot(handspan ~ height)]
survey[ , abline(lm(handspan ~ height))]
```

Note that `abline` can only be used *after* you've already made a plot. It _adds_ a line to the existing plot rather than making a plot of its own. 

You can also use `abline` to plot different kinds of lines. For example, we can show that the regression line goes through the means of the data as follows:

```{r abline_samplemean, results = 'hide'}
survey[ , {
  plot(handspan ~ height)
  abline(lm(handspan ~ height))
  abline(v = mean(height, na.rm = TRUE),
         h = mean(handspan, na.rm = TRUE),
         col = 'red', lty = 2)
}]
```

We've introduced a new piece of syntax in the `j` argument -- `{}`. This syntax allows us to run multiple commands within one pair of square brackets -- here, we're running `plot` and then `lm` twice. We'll see `{}` again below when we learn about writing functions; for now, observe that it's convenient not to have to write `survey[]` repeatedly.

As to the call to `abline`, we've supplied two new arguments. `v` indicates that we want a *vertical* line at the specified number(s), while `h` indicates that we want a *horizontal* line at the specified number(s).

We've also introduced some new plotting commands -- `lty` stands for _l_ine _ty_pe. `lty = 2` makes a dashed line. A standard line (the default) has `lty = 1`. Try some other line types and see what happens; the full description is in `?par`. `lty` can be used with any plot that features lines.

If you want to use `abline` to draw a line that is *neither* horizontal nor vertical, use the argument `a` for the intercept and `b` for the slope. For example:

```{r abline_ab, results = 'hide'}
x = seq(from = 0, to = 1, by = 0.1)
y = x^2
plot(y ~ x)
abline(a = 0, b = 1)
```

### Exercise #1 - Regression

Carry out a linear regression in which `height` is the y-variable and `handedness` is the x-variable. Plot the data and add the regression line. Add red dashed lines to show that the regression line goes through the means of the data. Interpret your results. Is there a relationship between handedness and height? 

```{r exercise_1, echo=FALSE, results='hide', fig.show='hide'}
survey[ , {
  reg = lm(height ~ handedness)
  plot(height ~ handedness)
  abline(reg = reg)
  abline(v = mean(handedness, na.rm = TRUE),
         h = mean(height, na.rm = TRUE),
         col = 'red', lty = 2)
}]
```

## Creating Your Own Functions {#part_two}

Functions take one or more inputs, called "arguments", and combine them in some way to produce an output. You've already met a great many of the built-in R functions, as well as several of the functions in the `data.table` package. Now it's time to make some of our own. Making functions is a great way to save time by automating repetitive tasks and extend the functionality of R to make it even more useful. 

### Simple Example

Suppose R didn't have a built-in function for computing z-scores (it does -- see `?scale`); given how much they come up in class, it would be great to use in our code something like `z.score(some_variable)` instead of having to compute it more manually each time. Luckily it's straightforward to do just that and build our own function! Among other things, this example will illustrate a key point about functions: you can build them out of *other functions*, which means you don't have to re-invent the wheel each time you want to do something. To make our z-score function we'll use the built-in R functions `mean` and `sd` rather than writing our own mean and standard deviation functions from scratch:

```{r function_write}
z.score = function(x){
  z = (x - mean(x, na.rm = TRUE))/sd(x, na.rm = TRUE)
  return(z)
}
```

Let's walk through the above code one step at a time:

1. In R `=`, pronounced "gets" is the assignment operator, so `z.score =` means that we're assigning something to `z.score`. This is just like assigning any other variable in R, except now this "variable" is a function named `z.score`.
2. There are some parentheses next to `function`. These are used to enclose the function's *arguments*. Argument is just another word for input. Functions can take one argument, more than one argument or no arguments depending on the situation, but you always need the parentheses. Here, `z.score = function(x)` means "R, I'm going to create a function called `z.score` that takes a single argument that I'm going to call `x`."
3. The next thing we see is a left curly bracket: `{`. Essentially, a function is a set of *instructions* that tells R how to turn some arguments (inputs) into a desired output. These instructions are called the function *body* and they are always enclosed in curly brackets. If you look at the bottom of the function, you'll see the second curly bracket. A common mistake that people make is to forget the second bracket. *Always make sure to close any bracket that you open*.
4. Now we'll look at the function body, the list of commands contained inside of the curly brackets. The first part should be familiar: it's made up entirely of R commands that you already know! It does the following:
  - Calculate the mean of the argument (input) `x`. Ignore missing observations.
  - Calculate the standard deviation the argument (input) `x`. Ignore missing observations.
  - Subtract the mean of `x` from the vector `x` and then divide by the standard deviation
  - Store the answer in a vector called `z`
5. The last part of the function is the command `return`. This indicates that our function should output or "return" the vector `z`. In other words, `z` is the "answer".  

Actually, step 5 is not strictly necessary -- R knows to return the output of whatever the last line of a function is. But it's safe to be explicit and let readers of your code know what your intended output of your function is.

It was easy to create the `z.score` function because, as we explored in R Tutorial #1, mathematical operations in R are *vectorized*. This means, for example, that if we want to add 3 to each element of a vector `x`, we can just write `x + 3`. In other words we don't have to *separately* add three to each element of `x`. Similarly, if we want to add corresponding elements of `x` and `y`, two vectors of the same length, we use `x+y`. Try some simple examples of your own to make sure you understand how this works. See what happens if you try to add two vectors of *different* lengths. 

### Scope
So what exactly is the status of `z` in the function `z.score`? It looks like an ordinary R variable and in a certain sense it is. But there is something slightly subtle here: `z` is a variable "inside a function", namely a variable inside the function `z.score`. This makes it different from the other variables that we've seen so far. To explain how, we'll need to introduce a programming concept called "scope."

Scope refers to where the variable "lives," meaning how it can be accessed. We will focus on two types of scope, *global* and *local*. 

If you type the command `ls()` into the R console, you will see a bunch of variables. These variables have "global scope". This means that they can be accessed from *anywhere* by *any function*. If you've run the code for the function `z.score` above, you'll see `z.score` when you type `ls()` into the console. This means that  `z.score` has global scope. Similarly if you type `foo = 3` into the R console, `foo` will have global scope. You can see this by re-running `ls()` after you create `foo`.

Notice, however, that you did *not* see `z` when you typed `ls()`. This is because `z` is a "variable inside a function." In other words, it has *local scope*. This means that `z` is only accessible *inside* the function `z.score`.


Another example of a variable with local is the column of a `data.table`, for example `handspan` in the dataframe `survey`. We can only access `handspan` by extracting it from `survey`, which has global scope, using `$` or `[]`. 

**Don't worry if you don't immediately understand this. It's a weird concept to grasp.** You can think of variables with global scope like books on a shelf. You can see each one and pick it out. In contrast, variables with local scope are like *chapters* in a book: you don't know what the chapter is called until you open a specific book and read it.

It's not at all required, but the [chapter on functions](http://adv-r.had.co.nz/Functions.html) from Hadley Wickham's free online advanced R textbook is the place to start looking for all the nitty-gritty details on how this works. _Caveat lector_ -- don't expect to understand much on your first pass through reading.

### Roll Your Own Mean

Let's look at another example: making a function to compute the sample mean. R has a perfectly good function for calculating means, as we've seen, so this is mainly for illustrative purposes*. I'll call this function `mymean` to distinguish it from the built-in R function:

*: Unnecessary aside: actually, the `mean` function in R will be _slower_ than this function for [subtle reasons explained here](http://stackoverflow.com/questions/18604406/why-is-mean-so-slow).

```{r mymean}
mymean = function(x){
  x = x[!is.na(x)]
  x.bar = sum(x)/length(x)
  return(x.bar)
}
```

Again, I'll walk you through this one step-by-step:

1. `mymean = function(x)` tells R to create a function that takes a single argument `x` and store it under the name `mymean`
2. The curly braces `{}` enclose the function *body*.
3. `x = x[!is.na(x)]` *overwrites* `x` with a new vector containing all the elements of `x` that are *not* `NA`s. In other words, this removes missing observations. (A subtle note about global/local scope here is that doing this _does not_ overwrite the _original_ vector that's passed to `mymean` as an argument -- it overwrites a _local copy_ called `x`)
4. `sum(x)/length(x)` calculates the sample mean: summing up the observations and dividing by the number of observations. Notice that I did *not* use `na.rm = TRUE` in the `sum` command. If I had used it, everything still would have worked but since I *already removed all the missing observations* in the line above, this wasn't necessary. 

Let's test out `my.mean` on some real data and make sure it works. First we'll use R's built-in `mean` function:

```{r mean_builtin}
mean(survey$height, na.rm = TRUE)
```

and then we'll try `mymean`

```{r mymean_test}
mymean(survey$height)
```

It works! Now I'm going to show you a slightly different version of `mymean` which I'll call `mymean2`. This one will *not* give the correct answer:

```{r mymean2}
mymean2 = function(x){
  x.bar = sum(x, na.rm = TRUE)/length(x)
  return(x.bar)
}
mymean2(survey$height)
```

The answer is too small! What's going on here? The problem has to do with how I handled missing observations. Rather than throwing them away, I told `sum` to ignore them. The problem is that `length` does *not* ignore them -- it simply counts up the number of elements in a vector, regardless of whether they're `NA`s. In effect we have divided by the *wrong* value of $n$: the sum is being taken over all *non-missing* observations while the length is being computed over *all* observations.

It's worth comparing `mymean2` to `z.score`, the first function I introduced above. In that example it was ok to use `na.rm = TRUE` to handle the missing observations since this argument was available for *both* the `mean` and `sd` functions. 

### Roll Your Own Variance

One more example before I set you loose to make your own function. Here's R code to create a function called `myvar` that calculates the sample variance. This function piggybacks on `mymean` from above. This is a **good thing**. We've already tested `mymean` and know that it works, so incorporating it into `myvar` is much better than starting from scratch. Again, note how we handle missing observations.

```{r myvar}
myvar = function(x){
  x = x[!is.na(x)]
  s.squared = sum((x-mymean(x))^2)/(length(x) - 1)
  return(s.squared)
}
```

We can test this against `var` to make sure it works:

```{r myvar_test}
var(survey$handspan, na.rm = TRUE)
myvar(survey$handspan)
```

Looks good!

### Exercise #2
It turns out that R doesn't have a built-in function to calculate skewness (there is one in the library `moments`). Your job is to write one. Remember the definition of skewness from lecture:
  $$\mbox{Skewness} = \frac{\frac{1}{n}\sum_{i=1}^n(x_i - \bar{x})^3}{s^3}$$
where $s$ is the sample standard deviation. Call your function `skew`. It should take one argument, a vector `x`, and return the sample skewness of `x`. Make sure to handle missing values appropriately. Use the built-in R functions `mean`, `sum`, `length` and `sd` to construct your skewness function. Test your function out on the `handedness` data. You should get an answer of around -2.2.  

```{r exercise_2, echo=FALSE, results='hide', fig.show='hide'}
#Exercise #2 - Write a Function to Calculate Skewness
skew = function(x){
  x = x[!is.na(x)]
  numerator = sum((x - mean(x))^3)/length(x)
  denominator = sd(x)^3
  return(numerator/denominator)  
}
skew(survey$handedness)
```

### Returning a `data.table`

The examples we've seen so far have been very simple: functions that take one argument and return a single value. In fact, functions can take multiple values and return pretty much anything. First we'll look at an example of a function that returns a `data.table`. This is a very flexible way of returning multiple values in a way that makes them easy to access.

```{r function_return_data.table}
summary.stats = function(x){
  x = x[!is.na(x)]
  sample.mean = mean(x)
  std.dev  = sd(x)
  out = data.table(sample.mean, std.dev)
  return(out)
}
results = summary.stats(survey$handedness)
results
results$sample.mean
results$std.dev
```

### Multiple Arguments

Now we'll look at a function that takes multiple arguments: `mycov`. This is essentially a stripped-down version of `cov`, the built-in R equivalent. Unlike `cov`, which can accept two vectors as arguments *or* a whole `data.table`, our version will only accept two vectors as arguments: `x` and `y`. Handling the missing observations is a bit more complicated in this situation: we need to drop any `x` for which there isn't a corresponding `y` and any `y` for which there isn't a corresponding `x` -- We've seen some examples of how to do this in R Tutorial #2 using `!is.na` and `&`. This is because covariance is a measure of the *relationship* between `x` and `y`. Accordingly, it is calculated using *pairs* of observations rather than *individual* observations. First I'll give you the code, then I'll explain the part that's different from what we did above: 

```{r mycov}
mycov = function(x, y){
  
  keep = !is.na(x) & !is.na(y)
  x = x[keep]
  y = y[keep]
  
  n = length(x)
  
  s.xy = sum( (x - mean(x)) * (y - mean(y)) ) / (n-1)
  return(s.xy)
}
```

Testing this, 

```{r mycov_test}
survey[ , cov(handspan, handedness, use = "complete.obs")]
survey[ , mycov(handspan, handedness)]
```

we see that it works. 

### Exercise #3

Write a function to carry out linear regression called `myreg`. It should take two arguments: vectors `x` and `y` which you may assume have the same length. It should return a `data.table` with columns `a` for the intercept and `b` for the slope. Make sure to handle missing values appropriately. Feel free to use any built-in R functions you like *except* `lm`. Check your answer against the regression results for height and handspan that we calculated above. When creating this function, remember that regression is *not* symmetric.

```{r exercise_3, echo=FALSE, results='hide', fig.show='hide'}
#Exercise #3 - Write a Function to Carry Out Linear Regression
myreg = function(y, x){
  keep = !is.na(x) & !is.na(y)
  x = x[keep]
  y = y[keep]
  
  b = cov(x,y)/var(x)
  a = mean(y) - b * mean(x)
    
  out = data.table(a, b)
  return(out)
}
survey[ , lm(height ~ handspan)]
survey[ , myreg(y = height, x = handspan)]
```

## Stock Market Data {#part_three}

Now that we know how to write our own functions, let's have a little fun playing the stock market. It's easy to download and play with stock price data using the R packages ``Quandl`` and ``zoo``. An R package is like an "expansion pack" that gives R new features. One of the great things about R is that there literally thousands of packages that expand the usefulness of R. 


#### Installing ``Quandl`` and ``zoo``
First you'll need to install ``Quandl`` and ``zoo``. You only need to do this once: all subsequent times you can skip this part. In RStudio go to "Tools > Install Packages." The first time you do this, you may be asked to choose a "Repository." Don't worry about what this is, you can choose any of them. The package installation will just be slightly quicker if you choose a repository that's geographically close to Penn. (It's really just a question of which server you instruct R to download the packages from.) Make sure that the checkbox "install dependencies" is checked and then enter ``Quandl, zoo`` in the text box labeled "Packages". Lots of text will whiz past in the R console and once it finishes you'll have access to the desired packages.

Again, you don't need to repeat the proceeding steps each time you want to use ``zoo`` or ``Quandl``. You do, however, need to *load* the packages every time you want to use them. (In other words, if you close RStudio you'll need to reload ``zoo`` and ``Quandl`` to use them.) Packages are loaded using the ``library`` command.
```{r}
library(Quandl)
library(zoo)
```
Now we're ready to get started.

#### Downloading Stock Prices
To start off I'll download prices for Apple stock:
```{r,cache=TRUE}
apple.prices = Quandl('GOOG/NASDAQ_AAPL', start_date = '2012-01-01', end_date = '2012-12-31', type = 'zoo')
```
Notice that I got a warning saying that my access to Quandl is limited. While the service is free, you have to sign up (also free), to get unlimited access. Otherwise you're only allowed a certain number of queries per day. For this exercise that won't be a problem.

Now I'll walk you through the arguments of the function ``Quandl``
  - The first argument is the *Quandl Code* for the data you want. In this case, it happens that the code for Apple is ``GOOG/NASDAQ_APPL``. The ``GOOG`` indicates that the data *source* is Google Finance and ``NASDAQ`` indicates that we're looking for a company that's traded on the NASDAQ stock exchange. Finally, ``APPL`` is the ticker symbol for Apple Computer.
  - ``start_date`` and ``end_date`` mean exactly what you think they do.
  - ``type = 'zoo'`` tells Quandl to set up our data in the right format for using the library ``zoo``. More on that below.

You can look up the codes for various datasets on the Quandl website. Here are a few others for you to play with:
  - Stock Prices for Google: ``GOOG/NASDAQ_GOOG``
  - Stock Prices for Amazon: ``GOOG/NASDAQ_AMZN``
  - Stock Prices for Netflix: ``GOOG/NASDAQ_NFLX``
  - S&P 500 Index Prices: ``YAHOO/INDEX_GSPC``
  
All of these series are at the daily frequency.

#### Plotting Daily Stock Prices for Apple
Now let's take a look at what's contained in ``apple.prices``
```{r}
head(apple.prices)
```
As you can see, there's quite a lot of information here: for each day we have the opening price, the high price, the low price and the closing price. ``Volume`` records how many shares were traded on that particular day.

The reason we loaded ``zoo`` is because it allows us to make some really nice plots with essentially no work. Since we instructed ``Quandl`` to store the Apple Computer price data in a format that ``zoo`` understands, the plot command now works almost like magic:
```{r}
plot(apple.prices)
```

We're mainly going to be interested in the closing prices, which we can plot by themselves as follows:
```{r}
plot(apple.prices$Close, main = 'Daily Closing Prices: Apple Computer - 2012', xlab = 'Date', ylab = 'Price')
```

### From Prices to Returns
In financial economics, we're usually more interested in stock *returns* than stock *prices*. Returns measure the *percentage change* in a stock's price, so they tell us how much the value of our investment has grown (or declined). If $p_t$ is today's stock price and $p_{t-1}$ is yesterday's stock price, then
$$r_t = \frac{p_t - p_{t-1}}{p_{t-1}}$$
is today's stock *return*. This is just the percentage change in price from yesterday to today *expressed as a decimal*. For example a return of $0.5$ corresponds to a 50% *increase* in the stock price and a return of $-0.05$ corresponds to a 5% *decrease* in the stock price.

#### Log Returns
As it happens, researchers in financial economics tend not to use the formula for *geometric returns* $r_t$ given in the previous section. Instead they work with something called *log returns* which is more convenient mathematically and very similar to geometric returns provided that $r_t$ isn't too large. The precise approximation is: $r_t \approx log(p_t) - log(p_{t-1})$ when $r_t$ is close to zero. In other words, we can approximate geometric returns by taking the *difference of the natural log of successive prices* rather than computing the ratio that defines $r_t$. The interpretation remains the same: a percentage change expressed as a decimal. 


**If you want to know more about log returns, there's an optional section at the end of this document explaining them in more detail.**


#### Log Returns for Apple
Now we'll construct daily log returns for Apple based on the closing prices. Remember: we need successive differences of the natural log of prices. The comand for successive differences in R is ``diff``, for example:
```{r}
foo = c(1,2,4,7,11)
diff(foo)
```
since $2-1 = 1$, $4-2 =2$, $7-4 = 3$ and $11 - 7 = 4$. We can combine this with the function ``log`` to compute log returns for Apple as follows:
```{r}
apple.returns = diff(log(apple.prices$Close))
```
Plotting these, we have
```{r}
plot(apple.returns, main = 'Apple Computer - 2012', xlab = 'Date', ylab = 'Log Daily Returns', col = 'blue')
```

Notice how different everything looks now! While the prices show a clear trend, the returns do not share this trend. They're much "spikier."" For those of you who know something about finance, this is often described by saying that prices follow a "random walk" while returns are "stationary." If you want to learn more about this, you should take Frank Diebold's undergrad course on forecasting.

#### Summary Statistics for Apple Returns
So what are the Apple's returns like? Could I make a fortune by owning this stock? The sample mean is a measure of *average* returns, which you can think of as the average "reward" for holding the stock
```{r}
mean(apple.returns)
```
While the sample standard deviation is a measure of the *variability* or *volatility* in returns 
```{r}
sd(apple.returns)
```
which you can think of as the "risk" of holding the stock. All things equal, you'd prefer a stock with high average returns and a low standard deviation.

Note that the preceding values are percentages *expressed as decimals*. To convert them into ordinary percents, we multiply by 100
```{r}
mean(apple.returns) * 100
sd(apple.returns) * 100
```
So, the average daily (log) return for Apple is 0.1% with a standard deviation of 1.8%. 

The nice thing about log returns is that to calculate cumulative returns over a period of time, you simply add up the daily log returns. For example, to get the cumulative log return for 2012 do the following:
```{r}
sum(apple.returns)
```
This means that if you'd started out holding $1 of Apple stock in January 2012, the value of your investment would have grown to about $1.26 by the end of the year.

#### Are Apple Returns Skewed? 
A classic question in financial economics is whether stock returns are skewed. Let's have a look at Apple:
```{r}
hist(apple.returns) 
mean(apple.returns)
median(apple.returns)
```




### Exercise #4
Are Apple Returns Skewed? Test out your function for calculating skewness on ``apple.returns`` and interpret your results. 
```{r echo=FALSE,results='hide',fig.show='hide',warning=FALSE}
#Exercise #4 - Are Apple Returns Skewed?
skew(apple.returns)
```



### Exercise #5
Download data for IBM (Quandl Code: GOOG/NYSE_IBM) and carry out the same analysis that we did for Apple Computer above. Use your results to answer the following questions:
  1. Which has higher average returns over the period studied: Apple or IBM?
  2. Which are more "volatile" or "risky": IBM's returns or Apple's?
  3. If you had invested a dollar in IBM at the start of 2012, how much would your investment have been worth at the end of the year?
  4. Are IBM returns skewed? How do they compare to Apple's returns in this sense?
  
```{r cache=TRUE,echo=FALSE,results='hide',fig.show='hide',warning=FALSE}
#Exercise #5 - Repeat the Above for IBM Returns
ibm.prices = Quandl("GOOG/NYSE_IBM", start_date = "2012-01-01", end_date = "2012-12-31", type = "zoo")
head(ibm.prices)
plot(ibm.prices)
plot(ibm.prices$Close, main = "Daily Closing Prices: IBM - 2012", xlab = "Date", ylab = "Price")
ibm.returns = diff(log(ibm.prices$Close))
plot(ibm.returns, main = "IBM - 2012", xlab = "date", ylab = "Log Daily Returns", col = "Blue")
mean(ibm.returns)
sd(ibm.returns)
sum(ibm.returns)
hist(ibm.returns)
mean(ibm.returns)
median(ibm.returns)
skew(ibm.returns)
```



### Exercise #6
In this exercise you'll calculate the *correlations* between different sets of returns.
  1. Calculate the sample correlation between Apple returns and IBM returns. Intepret your result.
  2. Download price data for Bank of America stock (Quandl Code: GOOG/NYSE_BAC) over the same period used for Apple and IBM and construct daily log returns using closing prices.
  3. Calculate the correlation between Apple and Bank of American returns, and Bank of America and IBM returns.
  4. Based on your answers to 1 and 3, which assets are most strongly correlated? Which are least strongly correlated? If wanted to "hedge your bets", which of these stocks would you choose to form a portfolio?
```{r cache=TRUE,echo=FALSE,results='hide',fig.show='hide',warning=FALSE}
#Exercise #6 - Correlations between Returns
boa.prices = Quandl("GOOG/NYSE_BAC", start_date = "2012-01-01", end_date = "2012-12-31", type = "zoo")
boa.returns = diff(log(boa.prices$Close))
cor(boa.returns, apple.returns)
cor(apple.returns, ibm.returns)
cor(boa.returns, ibm.returns)
```



### An Explanation of Log Returns (Optional)
You don't have to read this if you don't want to, and I'm not going to test you on it, but I suspect some of you may be curious about why log returns provide a good approximation of geometric returns and why summing log returns gives us cumulative returns. This section explains.

Recall the first-order Taylor Approximation from Calculus:

$$f(x) \approx f(x_0) + f'(x_0) (x - x_0)$$

In essence, this formula approximates an *arbitrary* differentiable function $f$ at a point $x$ by a *linear function* centered around a point of our own choosing, namely $x_0$. This is a good approximation *as long as $x$ is close to $x_0$*. To understand why, recall the definition of a derivative:

  $$f'(x_0) = \lim_{x \rightarrow x_0}\frac{f(x) - f(x_0)}{x- x_0}$$
  
So if $x$ is "close enough" to $x_0$,

  $$f'(x_0) \approx \frac{f(x) - f(x_0)}{x- x_0}$$
  
Rearranging this expression gives the Taylor Approximation. Now, consider the function $f(x) = log(1+x)$ where $log$ is the natural logarithm. Taylor expanding this around $x_0$ gives

  $$log(1+x) \approx  log(1+x_0) + \frac{1}{1+x_0} (x - x_0)$$
  
and the approximation is good when $x$ is close to $x_0$. We're free to choose any $x_0$ we want, so let's try $x_0 = 0$. Substituting, 

  $$log(1+x) \approx x$$
  
since $log(1)=0$. In other words, provided that $x$ is close to zero, $log(1+x)\approx x$. This is the approximation we'll use to motivate log returns. Now recall our definition of geometric returns from above:

$$r_t = \frac{p_{t}-p_{t-1}}{p_{t-1}}$$ 

This is equivalent to 

$$r_t = \frac{p_t}{p_{t-1}}-1$$

Applying the approximation $log(1+x) \approx x$ to $r_t$ and using the properties of logarithms

$$r_t \approx log(1 + r_t) = log(1 + p_t/p_{t-1} - 1) = log(p_t/p_{t-1}) = log(p_t) - log(p_{t-1})$$

In other words, the difference of log prices is approximately equal to $r_t$, provided that $r_t$ is close to zero. This is true of daily stock returns.

Now, suppose we wanted to calculate cumulative returns over some period of time $t =1, 2,..., T$. For example, suppose that we have daily stock returns and want to find out what the overall return was over a year. I asserted above that when we're working with log returns, all we have to do is sum them. Here's an explanation. Consider the sum of log returns:

$$S = \sum_{t=1}^T log(1 + r_t) = log(1+r_1) + log(1+r_2) +\dots + log(1+R_{T-1})+ log(1+ r_T)$$

Since $log(1+r_t) = log(p_t) - log(p_{t-1})$, we have

$$S = log(p_1) - log(p_0) + log(p_2) - log(p_1) + \dots + log(p_{T-1}) - log(p_{T-2}) + log(p_T) - log(p_{T-1})$$

But notice how almost all of these terms cancel out! All that remains is:

$$S = log(p_T) - log(p_0)$$

which is just the difference of the log price at the end of the year and the log price at the beginning of the year! Now, using the properties of logarithms as above:

$$S = log(p_T/p_0) = log\left(1 + \frac{p_T - p_0}{p_0} \right)$$

Now we immediately recognize $(p_T - p_0)/p_0$: this is the cumulative geometric return over the whole year: the percentage change in price between date 0 and date $T$. Call this quantity $R_T$. We see that the sum of log returns, $S$ is related to $R_T$ as follows:

$$S = log(1 + R_T)\approx R_T$$

again using our approximation from above. Note that this approximation only works when $R_T$ is relatively close to zero. Since it is a percentage expressed as a decimal, as long as the horizon over which we calculate cumulative returns isn't too long this is a reasonable approximation. For example, a cumulative return of 20% over one year gives $R_T = 0.2$ which is reasonably close to zero. In contrast a 400% return over 15 years corresponds to $R_T = 4$ which is not close to zero. Hence, the trick of summing log returns to approximate cumulative geometric returns works well as long as we don't look at time periods that are too long or stocks that grew (or declined) too fast.
